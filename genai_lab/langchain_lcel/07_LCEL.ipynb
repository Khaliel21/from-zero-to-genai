{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "hf_token_api = os.environ['HF_TOKEN']\n",
    "pinecone_api_key = os.environ['PINECONE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "Model = ChatGroq(\n",
    "    model=\"llama3-70b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RunnablePassthrough\n",
    "- it does not do anything to the input data.\n",
    "-a simple exemple chain with RunnablePassthrough() will output the original input without any modification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dan'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('Dan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableLambda\n",
    "- To use a custom function inside a LCEL chain we need to warp it up with RunnableLambda\n",
    "- let's make a simple function exemple create a filipino lastnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filipino_lastname(name : str) -> str:\n",
    "    return f\"Son {name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Son Suchesa'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filipino_lastname(\"Suchesa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "chain = RunnablePassthrough() | RunnableLambda(filipino_lastname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Son Suuchesa'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Suuchesa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableParallel\n",
    "\n",
    "- We will use `RunnableParallel()` for running tasks in parallel.\n",
    "- This is probably the most important and most useful `Runnable` from LangChain.\n",
    "- In the following chain, `RunnableParallel` is going to run these two tasks in parallel:\n",
    "  - `operation_a` will use `RunnablePassthrough`.\n",
    "  - `operation_b` will use `RunnableLambda` with the `filipino_lastname` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel({\n",
    "    'operation_a': RunnablePassthrough(),\n",
    "    'operation_b': RunnableLambda(filipino_lastname)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': 'Dan', 'operation_b': 'Son Dan'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Dan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Tell me a curious fact about {soccer_player}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filipino_lastname_from_dict(person):\n",
    "    return person['name'] + 'Son'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel({\n",
    "    'operation_a': RunnablePassthrough(),\n",
    "    'soccer_player': RunnableLambda(filipino_lastname_from_dict),\n",
    "    'operation_c' : RunnablePassthrough(),\n",
    "    \n",
    "}) | prompt | Model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I apologize, but I\\'m not familiar with a person or entity called \"NinaSon.\" Could you be referring to Nina Simone, the legendary American singer, songwriter, and civil rights activist? If so, here\\'s a curious fact about her:\\n\\nNina Simone was a classically trained pianist and was accepted into the Juilliard School of Music in New York City, but she was denied a scholarship to study at the Curtis Institute of Music in Philadelphia due to her race. This rejection was a pivotal moment in her life, leading her to focus on jazz and popular music instead.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    'name1' : 'Jordam',\n",
    "    \"name\": \"Nina\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advance use case of RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough ,RunnableParallel\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_embeddings = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_texts([\n",
    "    'Dan focuses on providing content on Data Science , AI, ML, DL, NLP, and python programming , ect, in English'], embedding = model_embeddings)\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question : {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "\n",
    "Model = ChatGroq(\n",
    "    model=\"llama3-70b-8192\"\n",
    ")\n",
    "\n",
    "retrieval_chain = (\n",
    "    RunnableParallel({'context': retriever, 'question': RunnablePassthrough()})\n",
    "    |prompt\n",
    "    |Model\n",
    "    |StrOutputParser()\n",
    "    \n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it can be inferred that Dan is a content provider, specifically focusing on providing content related to Data Science, AI, ML, DL, NLP, and Python programming.\n"
     ]
    }
   ],
   "source": [
    "print(retrieval_chain.invoke('what is Dan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use itemgetter from operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough ,RunnableParallel\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "vector_store = FAISS.from_texts([\n",
    "    'Dan focuses on providing content on Data Science , AI, ML, DL, NLP, and python programming , ect, in English'], embedding = model_embeddings)\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "Answer in the following language : {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "\n",
    "Model = ChatGroq(\n",
    "    model=\"llama3-70b-8192\"\n",
    ")\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\n",
    "        'context': itemgetter('question') | retriever,\n",
    "        'question': itemgetter('question'),\n",
    "        'language':  itemgetter('language'), \n",
    "    }\n",
    "    |prompt\n",
    "    |Model\n",
    "    |StrOutputParser()\n",
    "    \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'דן הוא מומחה בתחום המדעי הנתונים והתכנות.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke({'question': 'what is Dan', 'language': 'hebrew'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
