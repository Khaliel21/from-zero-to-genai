{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "groq_api_key = os.environ['GROQ_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Campletion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llamaChatModel = ChatGroq(\n",
    "    model=\"llama3-70b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt & Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for Completions Model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    'Tell me a {adjective} story about {topic}'\n",
    ")\n",
    "\n",
    "llmModelPromt = prompt_template.format(\n",
    "    adjective = \"curious\",\n",
    "    topic = 'mountain'\n",
    ")\n",
    "\n",
    "res = llamaChatModel.invoke(llmModelPromt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a curious story about a mountain:\n",
      "\n",
      "**The Mysterious Hum of Mount Tamalpais**\n",
      "\n",
      "In the heart of Marin County, California, stands Mount Tamalpais, a majestic mountain with a rich cultural and spiritual significance to the indigenous Coast Miwok people. For centuries, the mountain has been revered for its breathtaking beauty, diverse wildlife, and... its strange, unexplained hum.\n",
      "\n",
      "Locals and visitors alike have reported hearing a low, steady hum, often described as a gentle, pulsing drone, emanating from the mountain. The sound is usually heard at dawn and dusk, when the air is still, and can be loud enough to be felt as much as heard. Some claim it's like the mountain is \"singing\" or \"vibrating\" with an otherworldly energy.\n",
      "\n",
      "The first recorded mention of the hum dates back to the 1800s, when Native American tribes would gather at the mountain's base to perform rituals and ceremonies. They believed the hum was the mountain's way of communicating with them, sharing ancient wisdom and spiritual guidance.\n",
      "\n",
      "As the years passed, the hum continued to intrigue people. In the 1960s and 1970s, hippies and counterculture enthusiasts flocked to Mount Tamalpais, seeking enlightenment and spiritual connection. They believed the hum was a manifestation of the mountain's powerful energy, which could be harnessed for personal growth and transformation.\n",
      "\n",
      "Despite numerous scientific investigations, the source of the hum remains a mystery. Seismologists have ruled out earthquakes or geological activity as the cause, and sound engineers have been unable to pinpoint the exact location or frequency of the hum. Some have speculated that it might be related to the unique geology of the mountain, which is composed of a rare type of rock that's rich in quartz and other minerals.\n",
      "\n",
      "Others believe the hum is a manifestation of the mountain's spiritual energy, which has been awakened by the collective intentions and prayers of those who have visited the site over the centuries.\n",
      "\n",
      "To this day, the hum of Mount Tamalpais continues to fascinate and inspire those who venture to its slopes. Whether you're a scientist, a spiritual seeker, or simply a curious adventurer, the mountain's enigmatic hum is sure to leave you with a sense of wonder and awe.\n",
      "\n",
      "Have you ever heard of a similar phenomenon?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are an {profession} expert on {topic}.\"),\n",
    "    ('human', 'Hello Mr.{profession}, can you please answer a question?'),\n",
    "    ('ai', 'Sure!'),\n",
    "    (\"human\", '{user_input}'),\n",
    "])\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    profession = \"Historian\",\n",
    "    topic = 'The Kennedy family',\n",
    "    user_input ='How many grandchildren had Joseph P. Kennedy?' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llamaChatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy, the patriarch of the Kennedy family, had a total of 29 grandchildren. He had nine children with his wife Rose Fitzgerald Kennedy, and those children went on to have many children of their own. The 29 grandchildren of Joseph P. Kennedy include many notable figures, such as John F. Kennedy Jr., Caroline Bouvier Kennedy, Maria Shriver, and Patrick Kennedy, among others.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "exemple = [\n",
    "    {\"input\": 'Hi!', 'output': 'Hola'},\n",
    "    {\"input\": 'Bey!', 'output': 'adios'}\n",
    "]\n",
    "\n",
    "exemple_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('human', '{input}'),\n",
    "        ('ai', '{output}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=exemple_template,\n",
    "    examples=exemple\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',  'You are an English-Spanish translator'),\n",
    "        few_shot_prompt,\n",
    "        ('human', '{input}')\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estoy bien, ¿y tú?\n"
     ]
    }
   ],
   "source": [
    "chain = final_prompt | llamaChatModel # LangChain Expression Language\n",
    "\n",
    "res = chain.invoke({'input': \"How are you?\"})\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers For Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with an 'answer' key that answers the following question : {question}\"\n",
    ")\n",
    "\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "json_chain = json_prompt | llamaChatModel | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Russia'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_chain.invoke({'question': \"What is the biggest country?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optionally we can use Pydantic to define a custom output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1  import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic Object with the desired output format.\n",
    "class Joke(BaseModel):\n",
    "    setup : str = Field(description=\"question to set up a joke\")\n",
    "    punchline : str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template= \"Answer the user query. \\n{format_instruction}\\n{query}\\n\",\n",
    "    input_variables=['query'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()} ## answer base on the Custom format\n",
    ")\n",
    "\n",
    "chain = prompt | llamaChatModel | parser\n",
    "chain.invoke({\"query\": \"Tell me a Joke. \"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
