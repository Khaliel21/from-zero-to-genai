{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“„ Q&A Over PDF Files\n",
    "\n",
    "## ðŸ§© Introduction\n",
    "\n",
    "In this project, we will build a **Question & Answer (Q&A) application** that can respond to questions based on the content of **PDF documents**.\n",
    "\n",
    "### ðŸ§  How It Works\n",
    "- We use a **Document Loader** to extract and structure text from PDF files into a format that can be processed by an LLM (Large Language Model).\n",
    "- We then construct a **Retrieval-Augmented Generation (RAG)** pipeline, which allows the model to retrieve relevant information and generate accurate answers â€” including **citations from the source document** when needed.\n",
    "\n",
    "> **Note**: This project uses a basic implementation. In future projects, we will explore more advanced techniques for solving similar problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())\n",
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "hf_token_api = os.environ['HF_TOKEN']\n",
    "pinecone_api_key = os.environ['PINECONE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = r\"Data\\Be_Good.pdf\"\n",
    "\n",
    "docs = PyPDFLoader(pdf_path).load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "To implement RAG, we will follow a standard pipeline involving vector storage and text chunking.\n",
    "\n",
    "### ðŸ§± Components Used:\n",
    "\n",
    "- We will use **Chroma DB** as the **vector database** (also called a *vector store*).\n",
    "- A **text splitter** will be used to divide the loaded PDF into smaller chunks. These chunks are easier for the LLM to process within its context window.\n",
    "- Once split, the chunks are loaded into the vector store.\n",
    "\n",
    "### ðŸ”„ Retrieval\n",
    "\n",
    "We can then create a **retriever** from the vector store, which will be used in our **RAG chain** to find the most relevant content when answering user questions.\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ’¡ If you're using the pre-loaded Poetry shell, you don't need to manually install the required packages â€” theyâ€™re already included.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dan\\Desktop\\Python LLm Engineer\\01 Advanced ChatBot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "ccccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 0)\n",
    "\n",
    "chunk_of_text = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = FAISS.from_documents(documents=chunk_of_text, embedding=model_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(search_kwargs={'k':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# System prompt template\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "]\n",
    "\n",
    "# Define the full chat prompt\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# Create the document QA chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create the full RAG retrieval chain\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This article, \"Be Good\" by Paul Graham, discusses the principles of building a successful startup, focusing on creating something people want and not worrying too much about the business model initially. It explores the idea that this approach can lead to a charity-like mindset, where the goal is to help people, and provides examples of successful companies that have adopted this approach.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = rag_chain.invoke({\"input\" : \"What is this article about ?\"})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is this article about ?',\n",
       " 'context': [Document(metadata={'source': 'Data\\\\Be_Good.pdf', 'page': 0}, page_content=\"Be Good - Essay by Paul Graham\\nBe Good\\nBe good\\nApril 2008(This essay is derived from a talk at the 2008 Startup School.)About a month after we\\nstarted Y Combinator we came up with the\\nphrase that became our motto: Make something people want.  We've\\nlearned a lot since then, but if I were choosing now that's still\\nthe one I'd pick.Another thing we tell founders is not to worry too much about the\\nbusiness model, at least at first.  Not because making money is\\nunimportant, but because it's so much easier than building something\\ngreat.A couple weeks ago I realized that if you put those two ideas\\ntogether, you get something surprising.  Make something people want.\\nDon't worry too much about making money.  What you've got is a\\ndescription of a charity.When you get an unexpected result like this, it could either be a\\nbug or a new discovery.  Either businesses aren't supposed to be\\nlike charities, and we've proven by reductio ad absurdum that one\\nor both of the principles we began with is false.  Or we have a new\\nidea.I suspect it's the latter, because as soon as this thought occurred\\nto me, a whole bunch of other things fell into place.ExamplesFor example, Craigslist.  It's not a\\ncharity, but they run it like\\none.  And they're astoundingly successful.  When you scan down the\\nlist of most popular web sites, the number of employees at Craigslist\\nlooks like a misprint. Their revenues aren't as high as they could\\nbe, but most startups would be happy to trade places with them.In Patrick O'Brian's novels, his\\nPage 1\"),\n",
       "  Document(metadata={'source': 'Data\\\\Be_Good.pdf', 'page': 10}, page_content=\"Be Good - Essay by Paul Graham\\nGoogle does.Most explicitly benevolent projects don't hold themselves sufficiently\\naccountable.  They act as if having good intentions were enough to\\nguarantee good effects.[3] Users dislike their\\nnew operating system so much that they're starting petitions to\\nsave the old one.  And the old one was nothing special.  The hackers\\nwithin Microsoft must know in their hearts that if the company\\nreally cared about users they'd just advise them to switch to OSX.Thanks to Trevor Blackwell, Paul\\nBuchheit, Jessica Livingston,\\nand Robert Morris for reading drafts of this.\\nPage 11\"),\n",
       "  Document(metadata={'source': 'Data\\\\Be_Good.pdf', 'page': 5}, page_content=\"Be Good - Essay by Paul Graham\\nyou've basically built yourself a giant tamagotchi.  You've made\\nsomething you need to take care of.Blogger is a famous example of a startup that went through\\nreally\\nlow lows and survived.  At one point they ran out of money and\\neveryone left. Evan Williams came in to work the next day, and there\\nwas no one but him.  What kept him going?  Partly that users needed\\nhim.  He was hosting thousands of people's blogs. He couldn't just\\nlet the site die.There are many advantages of launching quickly, but the most important\\nmay be that once you have users, the tamagotchi effect kicks in.\\nOnce you have users to take care of, you're forced to figure out\\nwhat will make them happy, and that's actually very valuable\\ninformation.The added confidence that comes from trying to help people can\\nalso help you with investors. One of the founders of \\nChatterous told \\nme recently that he and his cofounder had decided that this service\\nwas something the world needed, so they were going to keep working\\non it no matter what, even if they had to move back to Canada and live\\nin their parents' basements.Once they realized this, they stopped caring so much what investors\\nthought\\nabout them.  They still met with them, but they weren't going to\\ndie if they didn't get their money.  And you know what?  The investors\\ngot a lot more interested.  They could sense that the Chatterouses\\nwere going to do this startup with or without them.If you're really committed and your startup is\\ncheap to run, you\\nbecome very hard to kill.  And practically all startups, even the\\nPage 6\")],\n",
       " 'answer': 'This article, \"Be Good\" by Paul Graham, discusses the principles of building a successful startup, focusing on creating something people want and not worrying too much about the business model initially. It explores the idea that this approach can lead to a charity-like mindset, where the goal is to help people, and provides examples of successful companies that have adopted this approach.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
